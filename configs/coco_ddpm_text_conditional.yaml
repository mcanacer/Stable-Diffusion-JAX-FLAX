dataset_params:
  dataset: 'coco'
  img_size: 128
  batch_size: 64
  num_workers: 4
  params:
    dataset_dir: /content/train2017
    ann_file_path: /content/annotations/captions_train2017.json
    max_length: 77

model:
  learning_rate: 2e-5
  seed: 61
  target: 'UNet'
  condition_config:
    types: ['text']
    text_config:
      model_name: 'openai/clip-vit-base-patch32'
      text_drop_prob: 0.1
  params:
    channel: 128
    channel_multiplier: [1, 2, 2, 4, 4]
    n_res_block: 2
    attn_strides: [8, 16]
    attn_heads: 1
  checkpoint_path: /content/drive/MyDrive/Stable-Diffusion/coco_text2image_diffusion.pkl
  epochs: 200
  ema_decay: 0.9999

sampler:
  sampler_type: 'DDPM'
  params:
    total_timesteps: 1000
    beta_start: 0.00085
    beta_end: 0.012
    schedule_type: 'linear'

vqmodel:
  target: model.VQModel
  params:
    embedding_dim: 4
    num_embeddings: 16384
    output_channels: 3
    commitment_cost: 0.25
    channel_multipliers: [1, 2, 2, 4]
    attn_resolutions: [32]
    n_heads: 4
  checkpoint_path: /content/drive/MyDrive/Stable-Diffusion/vqmodel_coco.pkl

wandb:
  project: 'Stable-Diffusion'
  name: 'Diffusion Coco-Text2Image'
